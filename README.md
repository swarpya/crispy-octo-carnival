# RAG System - Complete Implementation: Document Retrieval & AI Response Generation

This is the complete implementation of a RAG (Retrieval-Augmented Generation) system that processes books, stores them in a vector database, and provides intelligent AI-powered responses using Groq LLM.

## Project Structure

```
rag_system/
‚îú‚îÄ‚îÄ config.py             # Configuration settings (updated for Phase 3)
‚îú‚îÄ‚îÄ models.py             # Data models (Book, TextChunk)
‚îú‚îÄ‚îÄ text_extractor.py     # Text extraction from PDFs/TXT files
‚îú‚îÄ‚îÄ chunker.py            # Text chunking with overlap
‚îú‚îÄ‚îÄ embedder.py           # Embedding generation
‚îú‚îÄ‚îÄ vector_store.py       # Qdrant vector database operations
‚îú‚îÄ‚îÄ query_processor.py    # Query processing and search (Phase 2)
‚îú‚îÄ‚îÄ response_generator.py # AI response generation (Phase 3)
‚îú‚îÄ‚îÄ phase1_main.py        # Phase 1: Data preparation pipeline
‚îú‚îÄ‚îÄ phase2_main.py        # Phase 2: Interactive query interface
‚îú‚îÄ‚îÄ phase3_main.py        # Phase 3: Complete RAG with AI responses
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies (updated)
‚îú‚îÄ‚îÄ books/                # Directory for book files
‚îî‚îÄ‚îÄ processed_data/       # Directory for processed data
```

## Quick Start

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up Groq API Key**
   ```bash
   export GROQ_API_KEY="your_groq_api_key_here"
   ```

3. **Start Qdrant**
   ```bash
   docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
   ```

4. **Add Books & Run Complete Pipeline**
   ```bash
   # Add PDF/TXT files to books/ directory
   python phase1_main.py    # Process books
   python phase3_main.py    # Run complete RAG system
   ```

## System Overview

### Phase 1: Data Preparation
- **Book Discovery**: Scans `books/` directory for PDF and TXT files
- **Text Extraction**: Extracts text page by page with metadata preservation
- **Text Chunking**: Creates 400-word chunks with 50-word overlap
- **Embedding Generation**: Uses sentence-transformers for semantic embeddings
- **Vector Storage**: Stores in Qdrant with full metadata

### Phase 2: Query Processing & Search
- **Semantic Search**: Natural language query processing
- **Relevance Ranking**: Cosine similarity scoring
- **Filtered Search**: Book-specific and author-specific queries
- **Context Assembly**: Intelligent context extraction for responses

### Phase 3: AI-Powered Response Generation ‚≠ê
- **LLM Integration**: Uses Groq's Llama-4-Scout model for responses
- **Context-Aware Answers**: Generates responses based on retrieved documents
- **Source Citations**: Automatically cites sources in responses
- **Streaming Support**: Real-time response generation
- **Smart Prompting**: Optimized prompts for accurate, helpful responses

## Usage Examples

### Complete AI-Powered Q&A
```bash
$ python phase3_main.py
ü§î Your question: What is quantum entanglement and how does it work?

üîç Searching for: 'What is quantum entanglement and how does it work?'
üìä Found 8 relevant sources
ü§ñ Generating AI response...

üí≠ AI Response:
Quantum entanglement is a phenomenon in quantum physics where two or more particles become interconnected in such a way that the quantum state of each particle cannot be described independently...

üìö Sources:
1. Quantum Physics Fundamentals by Dr. Sarah Chen (Page 45)
2. Modern Physics Concepts by Prof. Robert Johnson (Page 112)
3. Quantum Mechanics Explained by Dr. Maria Rodriguez (Page 78)
```

### Streaming Responses
```bash
ü§î Your question: stream How do neural networks learn?
```

### Search-Only Mode
```bash
ü§î Your question: sources machine learning algorithms
```

## Available Commands

| Command | Description | Example |
|---------|-------------|---------|
| `<question>` | Get AI-powered answer | `What is photosynthesis?` |
| `stream <question>` | Get streaming AI response | `stream How do computers work?` |
| `sources <question>` | Show search results only | `sources artificial intelligence` |
| `stats` | Show collection statistics | `stats` |
| `help` | Display help information | `help` |
| `quit` | Exit the system | `quit` |

## Configuration

### LLM Settings (Phase 3)
```python
GROQ_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"
GROQ_TEMPERATURE = 0.5  # Controls response creativity
GROQ_MAX_TOKENS = 1024  # Maximum response length
```

### Search & Retrieval Settings
```python
DEFAULT_TOP_K = 10              # Results to retrieve
DEFAULT_SCORE_THRESHOLD = 0.5   # Minimum relevance score
CHUNK_SIZE = 400                # Words per text chunk
CHUNK_OVERLAP = 50              # Overlap between chunks
```

## Key Features

### Smart Response Generation
- **Context-Aware**: Responses based on your specific document collection
- **Source Citations**: Every answer includes source references
- **Comprehensive**: Combines information from multiple relevant sources
- **Accurate**: Built-in safeguards against hallucination

### Advanced Search Capabilities
- **Semantic Understanding**: Finds relevant content even with different wording
- **Multi-Modal Queries**: Supports questions, keywords, and complex queries
- **Relevance Ranking**: Results sorted by semantic similarity
- **Metadata Filtering**: Search by book, author, or content type

### Performance Optimizations
- **Streaming Responses**: Real-time answer generation
- **Efficient Retrieval**: Optimized vector search with Qdrant
- **Batch Processing**: Efficient document processing pipeline
- **Memory Management**: Optimized for large document collections

## Troubleshooting

### Setup Issues
**Groq API Error:**
- Ensure `GROQ_API_KEY` environment variable is set
- Check API key validity at groq.com

**No AI Responses:**
- Verify internet connection for Groq API access
- Check API quota and rate limits
- Ensure Phase 1 completed successfully

### Search Issues
**No Results Found:**
- Run `python phase1_main.py` to process books
- Check books are in correct format in `books/` directory
- Try different query phrasing

**Poor Response Quality:**
- Adjust `GROQ_TEMPERATURE` (lower = more focused)
- Increase `DEFAULT_TOP_K` for more context
- Ensure books contain relevant information

## API Usage (Programmatic)

```python
from query_processor import QueryProcessor
from response_generator import ResponseGenerator

# Initialize components
query_processor = QueryProcessor()
response_generator = ResponseGenerator()

# Complete RAG pipeline
query_result = query_processor.process_query("What is artificial intelligence?")
ai_response = response_generator.generate_response(query_result)

print(f"AI Response: {ai_response}")
print(f"Based on {len(query_result.results)} sources")
```

## System Requirements

- **Python**: 3.8 or higher
- **Memory**: 8GB RAM recommended (4GB minimum)
- **Storage**: 1GB free space per 100MB of books
- **Internet**: Required for Groq API access
- **Docker**: For Qdrant vector database

## Next Steps (Future Enhancements)

- **Phase 4**: Web interface with REST API
- **Phase 5**: Multi-modal support (images, tables)
- **Advanced Features**: Conversation memory, query refinement, custom models

## Performance Benchmarks

| Collection Size | Search Time | Response Time | Memory Usage |
|----------------|-------------|---------------|--------------|
| 10 books | ~0.1s | ~2-5s | ~2GB |
| 50 books | ~0.2s | ~3-6s | ~4GB |
| 100 books | ~0.3s | ~4-7s | ~6GB |

## Contributing

When extending the system:
1. Follow the modular architecture pattern
2. Add comprehensive error handling
3. Include logging for debugging
4. Update configuration options
5. Add usage examples and tests

## License & Usage

This RAG system is designed for educational and research purposes. Please ensure compliance with:
- Groq API terms of service
- Document copyright restrictions
- Data privacy requirements

---

**Ready to get started?** Run `python phase3_main.py` and start asking questions about your documents! üöÄ